{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a955cdce-2c2c-4fed-ab60-fe1f4a8c2bb0",
   "metadata": {},
   "source": [
    "# Crawling to SQLite3\n",
    "\n",
    "This code walks through how to crawl and save crawled data in sql database. In detail, for a given url,\n",
    "1. this code navigates all webpages inside.\n",
    "2. for each webpage, collect the following information: title of the page; description; the number of external links; page contents; canonical.\n",
    "3. the info on each webpage is saved as a row in database.\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1feb00-9c42-4241-9bb0-4829b0f450d5",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c231657-36fa-44a6-bbcc-fbbccb5c8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import requests\n",
    "from datetime import date\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers= {'User-agent': 'Mozilla/5.0'}\n",
    "date = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef816f22-9ab1-48cd-af87-533455b10258",
   "metadata": {},
   "source": [
    "## Connecting to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "886bdff3-a09e-4dca-bf2c-50108d1d95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= sqlite3.connect('site_crawl.db')\n",
    "cursor= db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210ecc4-381e-402f-8de2-ccfe5352652d",
   "metadata": {},
   "source": [
    "### Create the master table to keep the list of basic urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8c56aa8-63f2-472a-8d60-a6bd9eb56f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x222ebef3420>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS Crawled_site_list (\n",
    "                    URLID INTEGER PRIMARY KEY AUTOINCREMENT, \n",
    "                    url varchar(255), \n",
    "                    date varchar(255),\n",
    "                    number_webpages_crawled integer\n",
    "                )\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9713630-a319-40b8-95be-3a1f6b7e15c3",
   "metadata": {},
   "source": [
    "### Get a basic url and create its database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4386c0fb-3e3d-4326-9c12-f8588bab2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "URL to crawl:  https://www.sec.gov\n"
     ]
    }
   ],
   "source": [
    "url = input(\"URL to crawl: \") # the exact url (incl. http or https and www. if any) should be typed.\n",
    "if len(url) < 1:\n",
    "    url = \"http://charlieojackson.co.uk\"\n",
    "    \n",
    "def get_db_name(url):\n",
    "    \"\"\"Takes a URL and strips it to use as a table name\"\"\"    \n",
    "    url_clense = re.findall(r'(ht.*://)?(www\\.)?(.*?)\\.', url)\n",
    "    url_clense = url_clense[0][-1].capitalize()\n",
    "    return url_clense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aaa39532-85f5-459f-bb4d-2ffcfb10fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov\n"
     ]
    }
   ],
   "source": [
    "db_name= get_db_name(url)\n",
    "cursor.execute(f\"\"\"DROP TABLE IF EXISTS \"\"\" + db_name)\n",
    "\n",
    "cursor.execute(f\"\"\"CREATE TABLE {db_name}\n",
    "               (URLID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               URL varchar(255),\n",
    "               Title varchar(255),\n",
    "               Description varchar(255),\n",
    "               InternalLinks INTEGER,\n",
    "               ExternalLinks INTEGER, \n",
    "               PageContents TEXT, \n",
    "               Canonical varchar(255), \n",
    "               Time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "               )\n",
    "               \"\"\")\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4a03faf-59a3-4e9f-9168-8c86c909cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "all_urls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94549f-0fb4-481b-803a-5fb38e76c348",
   "metadata": {},
   "source": [
    "## Functions to extract data from HTML source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bb715ae-1c4b-4d48-b79f-d0c216225f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(soup):\n",
    "    \"\"\"Extract required data for crawled page\"\"\"\n",
    "    title= soup.title.string\n",
    "    try:\n",
    "        description= soup.find(\"meta\", {\"name\":\"description\"})['content']\n",
    "    except:\n",
    "        description=\"Null\"\n",
    "        \n",
    "    try:\n",
    "        canonical = soup.find('link', {'rel':'canonical'})['href']\n",
    "    except:\n",
    "        canonical = \"Null\"\n",
    "        \n",
    "    contents_dirty= soup.text\n",
    "    contents=contents_dirty.replace(\"\\n\",\"\")\n",
    "    return (title, description, contents, canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6302cf7f-b11f-4383-b593-c4e95644cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(soup):\n",
    "    \"\"\"Extract links and link contents from page\"\"\"\n",
    "    links_dirty= soup.find_all('a')\n",
    "    for link in links_dirty:\n",
    "        if str(link.get('href')).startswith(url)== True and link.get('href') not in all_urls:\n",
    "            if '.jpg' in link.get('href') or '.png' in link.get('href'):\n",
    "                continue\n",
    "            else:\n",
    "                all_urls.append(link.get('href'))\n",
    "            \n",
    "    return(len(links_dirty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "628b573b-8f00-4468-b034-d7d751d703d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(extracted_data):\n",
    "    \"\"\"Insert the crawled data into the database\"\"\" \n",
    "    url, title, description, contents, no_of_links, canonical= extracted_data\n",
    "\n",
    "    cursor.execute(\"INSERT INTO \" + db_name + \" (URL, Title, Description, ExternalLinks, PageContents, Canonical) VALUES(?,?,?,?,?,?)\",(url, title, description, no_of_links, contents, canonical))\n",
    "\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dad16-d5d8-40e5-87c2-d5c714b71906",
   "metadata": {},
   "source": [
    "## Starting the web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "866a63f8-9d6c-49c6-8764-6b36f9ab9725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 crawling: https://www.sec.gov\n",
      "1 crawling: https://www.sec.gov/litigation.shtml\n",
      "2 crawling: https://www.sec.gov/divisions/enforce/claims.htm\n",
      "3 crawling: https://www.sec.gov/litigation/litreleases/2022/lr25298.htm\n",
      "4 crawling: https://www.sec.gov/litigation/litreleases/2021/lr25297.htm\n"
     ]
    }
   ],
   "source": [
    "link_counter= 0\n",
    "while link_counter < len(all_urls):\n",
    "    try:\n",
    "        print(str(link_counter) + \" crawling: \" + all_urls[link_counter])\n",
    "        r = requests.get(all_urls[link_counter], headers= headers)\n",
    "        if r.status_code== 200:\n",
    "            html= r.text\n",
    "            soup= BeautifulSoup(html, \"html.parser\")\n",
    "            no_of_links=extract_links(soup)\n",
    "            title, description, contents, canonical = extract_content(soup)\n",
    "            extracted_data= (all_urls[link_counter], title, description, contents, no_of_links,canonical)\n",
    "            insert_data(extracted_data)\n",
    "        link_counter += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        link_counter += 1\n",
    "        print(str(e))\n",
    "        \n",
    "    if link_counter>=5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c936f8-ffc2-4427-bb7a-146779348606",
   "metadata": {},
   "source": [
    "### Add the basic url to the master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5484a7b-9143-47fa-847c-6430f1807be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"INSERT INTO Crawled_site_list (url, date, number_webpages_crawled) VALUES(?, ?, ?)\"\"\", (url, date, link_counter))\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee242cc1-4a93-455c-9ffe-0fd003207325",
   "metadata": {},
   "source": [
    "### Display the list of basic urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69eaef40-b3bb-422d-b287-7925c66f0fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'https://www.cnbc.com', '2022-01-04', 5),\n",
       " (2, 'http://charlieojackson.co.uk', '2022-01-04', 5),\n",
       " (3, 'https://www.sec.gov', '2022-01-04', 5)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"SELECT * from  Crawled_site_list\"\"\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b923d0c-cba4-4a14-92c7-a0b7f491d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sec",
   "language": "python",
   "name": "sec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
